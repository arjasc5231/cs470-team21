{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "premium"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "모델 학습을 위한 데이터셋을 생성"
      ],
      "metadata": {
        "id": "rXNqlYIVGWTT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OlfYjwYIF7kI",
        "outputId": "16966072-f826-4d21-815c-8a1892b1c49a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers &> /dev/null\n",
        "from transformers import BertTokenizer\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "import csv\n",
        "import gzip\n",
        "import json\n",
        "import pickle\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "NA7QjZGDGAGW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_amazon_from_json(json_filename = 'AMAZON_FASHION_5.json'):\n",
        "  review_texts = []\n",
        "  ratings = []\n",
        "  products = []\n",
        "\n",
        "  root = '/content/drive/MyDrive/CS470_team_2in1'\n",
        "  with open(root+'/'+'dataset'+'/'+json_filename, \"r\") as json_file:\n",
        "    for line in json_file:\n",
        "      info = json.loads(line)\n",
        "\n",
        "      try:\n",
        "        review_text = info[\"reviewText\"]\n",
        "        rating = int(info[\"overall\"])\n",
        "        product = info[\"asin\"]\n",
        "      except:\n",
        "        continue\n",
        "      \n",
        "      review_texts.append(review_text)\n",
        "      ratings.append(rating)\n",
        "      products.append(product)\n",
        "\n",
        "  print(f'collected {len(review_texts)} datas')\n",
        "  return review_texts, ratings, products"
      ],
      "metadata": {
        "id": "VfY5ZjkTGDO_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_amazon_from_gzip(json_filename = 'Books_5.json.gz', data_num=None, start_idx=0):\n",
        "  review_texts = []\n",
        "  ratings = []\n",
        "  products = []\n",
        "\n",
        "  root = '/content/drive/MyDrive/CS470_team_2in1'\n",
        "  idx = 0\n",
        "  with gzip.open(root+'/'+'dataset'+'/'+json_filename, \"rb\") as f:\n",
        "    for line in f:\n",
        "      idx+=1\n",
        "      if start_idx>idx: continue\n",
        "      \n",
        "      info = json.loads(line)\n",
        "\n",
        "      try:\n",
        "        review_text = info[\"reviewText\"]\n",
        "        rating = int(info[\"overall\"])\n",
        "        product = info[\"asin\"]\n",
        "      except:\n",
        "        continue\n",
        "      \n",
        "      review_texts.append(review_text)\n",
        "      ratings.append(rating)\n",
        "      products.append(product)\n",
        "\n",
        "      if data_num and len(review_texts)==data_num: break\n",
        "      if len(review_texts)%100000==0: print(f\"processed {len(review_texts)} datas...\")\n",
        "\n",
        "  print(f'collected {len(review_texts)} datas')\n",
        "  return review_texts, ratings, products"
      ],
      "metadata": {
        "id": "aSJ5pf-hOHwT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_tokenized_dataset(review_texts, ratings, products):\n",
        "  tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
        "\n",
        "  input_ids = []\n",
        "  attention_masks = []\n",
        "\n",
        "  for sent in review_texts:\n",
        "      encoded_dict = tokenizer.encode_plus(\n",
        "                          sent, \n",
        "                          add_special_tokens = True,\n",
        "                          max_length = 512,\n",
        "                          truncation=True,\n",
        "                          pad_to_max_length = True,\n",
        "                          return_attention_mask = True,\n",
        "                          return_tensors = 'pt',\n",
        "                    )\n",
        "      \n",
        "      input_ids.append(encoded_dict['input_ids'])\n",
        "      attention_masks.append(encoded_dict['attention_mask'])\n",
        "\n",
        "      if len(input_ids)%100000==0: print(f\"tonkenized {len(input_ids)} datas...\")\n",
        "\n",
        "  dataset = np.array([input_ids, attention_masks, ratings, products])\n",
        "  print(\"dataset is gernerated\")\n",
        "  return dataset\n"
      ],
      "metadata": {
        "id": "CBMZc9CmbeKs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "review_texts, ratings, products = preprocess_amazon_from_gzip('Books_5.json.gz',data_num=500000)\n",
        "dataset = generate_tokenized_dataset(review_texts, ratings, products)\n",
        "np.save(\"/content/drive/MyDrive/CS470_team_2in1/dataset\"+\"/\"+\"amazon_book_only500000.npy\", dataset)"
      ],
      "metadata": {
        "id": "b7GWHrwQOjOa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"/content/drive/MyDrive/CS470_team_2in1/dataset/\"+'preprocessed_balanced_encoded.csv', 'r') as csvfile:\n",
        "    csvreader = csv.reader(csvfile)\n",
        "\n",
        "    reviewText = []\n",
        "    overall = []\n",
        "    productId = []\n",
        "\n",
        "    fields = ['index', 'sentence index', 'reviewText', 'overall', 'productID', 'positive rationale', 'negative rationale', 'positive rationale encoded', 'negative rationale encoded']\n",
        "    \n",
        "    for i, row in enumerate(csvreader):\n",
        "        if(i==0): continue\n",
        "        reviewText.append(row[2])\n",
        "        overall.append(int(row[3]))\n",
        "        productId.append(row[4])\n",
        "\n",
        "dataset = generate_tokenized_dataset(reviewText, overall, productId)\n",
        "np.save(\"/content/drive/MyDrive/CS470_team_2in1/dataset\"+\"/\"+\"rationale_preprocessed.npy\", dataset)"
      ],
      "metadata": {
        "id": "icZm6YLL0EVQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}