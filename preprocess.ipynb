{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "premium"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "모델 학습을 위한 데이터셋을 생성"
      ],
      "metadata": {
        "id": "rXNqlYIVGWTT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OlfYjwYIF7kI"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers &> /dev/null\n",
        "from transformers import BertTokenizer\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "import csv\n",
        "import gzip\n",
        "import json\n",
        "import pickle\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "NA7QjZGDGAGW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_amazon(json_filename = 'Books_5.json.gz', data_num=None, start_idx=0):\n",
        "  review_texts = []\n",
        "  ratings = []\n",
        "  products = []\n",
        "  pos=0\n",
        "  neg=0\n",
        "  root = '/content/drive/MyDrive/CS470_team_2in1'\n",
        "  idx = 0\n",
        "  with gzip.open(root+'/'+'dataset'+'/'+json_filename, \"rb\") as f:\n",
        "    for line in f:\n",
        "      idx+=1\n",
        "      if start_idx>idx: continue\n",
        "      \n",
        "      info = json.loads(line)\n",
        "\n",
        "      try:\n",
        "        review_text = info[\"reviewText\"]\n",
        "        rating = int(info[\"overall\"])\n",
        "        product = info[\"asin\"]\n",
        "      except:\n",
        "        continue\n",
        "      if (rating>3 and pos<data_num/2):\n",
        "        review_texts.append(review_text)\n",
        "        ratings.append(rating)\n",
        "        products.append(product)\n",
        "        pos+=1\n",
        "      elif (rating < 3 and neg < data_num/2):\n",
        "        review_texts.append(review_text)\n",
        "        ratings.append(rating)\n",
        "        products.append(product)\n",
        "        neg+=1\n",
        "\n",
        "      if data_num and len(review_texts)==data_num: \n",
        "        print(f'index {idx}')\n",
        "        break\n",
        "      if len(review_texts)%1000==0: \n",
        "        print(f\"processed {len(review_texts)} datas...\")\n",
        "        print(f'postive: {pos} negative: {neg}')\n",
        "\n",
        "  print(f'collected {len(review_texts)} datas')\n",
        "  return review_texts, ratings, products"
      ],
      "metadata": {
        "id": "Tg5x4K3KWADU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_tokenized_dataset(review_texts, ratings, products):\n",
        "  tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
        "\n",
        "  input_ids = []\n",
        "  attention_masks = []\n",
        "\n",
        "  for sent in review_texts:\n",
        "      encoded_dict = tokenizer.encode_plus(\n",
        "                          sent, \n",
        "                          add_special_tokens = True,\n",
        "                          max_length = 512,\n",
        "                          truncation=True,\n",
        "                          pad_to_max_length = True,\n",
        "                          return_attention_mask = True,\n",
        "                          return_tensors = 'pt',\n",
        "                    )\n",
        "      \n",
        "      input_ids.append(encoded_dict['input_ids'])\n",
        "      attention_masks.append(encoded_dict['attention_mask'])\n",
        "\n",
        "      if len(input_ids)%100000==0: print(f\"tonkenized {len(input_ids)} datas...\")\n",
        "\n",
        "  dataset = np.array([input_ids, attention_masks, ratings, products])\n",
        "  print(\"dataset is gernerated\")\n",
        "  return dataset\n"
      ],
      "metadata": {
        "id": "CBMZc9CmbeKs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "review_test,rating_test,product_test = preprocess_amazon(data_num=100000,start_idx = 10000)\n",
        "dataset_train = generate_tokenized_dataset(review_train, rating_train, product_train)\n",
        "np.save(\"/content/drive/MyDrive/CS470_team_2in1/dataset\"+\"/\"+\"amazon_book_train_rev.npy\", dataset_train)\n",
        "\n",
        "review_test,rating_test,product_test = preprocess_amazon(data_num=20000,start_idx = 600000)\n",
        "dataset_val = generate_tokenized_dataset(review_val, rating_val, product_val)\n",
        "np.save(\"/content/drive/MyDrive/CS470_team_2in1/dataset\"+\"/\"+\"amazon_book_val_rev.npy\", dataset_val)\n",
        "\n",
        "review_test,rating_test,product_test = preprocess_amazon(data_num=20000,start_idx = 700000)\n",
        "dataset_test = generate_tokenized_dataset(review_test, rating_test, product_test)\n",
        "np.save(\"/content/drive/MyDrive/CS470_team_2in1/dataset\"+\"/\"+\"amazon_book_test_rev.npy\", dataset_test)"
      ],
      "metadata": {
        "id": "IAgBnjhyaStC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"/content/drive/MyDrive/CS470_team_2in1/dataset/\"+'preprocessed_balanced_encoded.csv', 'r') as csvfile:\n",
        "    csvreader = csv.reader(csvfile)\n",
        "\n",
        "    reviewText = []\n",
        "    overall = []\n",
        "    productId = []\n",
        "\n",
        "    fields = ['index', 'sentence index', 'reviewText', 'overall', 'productID', 'positive rationale', 'negative rationale', 'positive rationale encoded', 'negative rationale encoded']\n",
        "    \n",
        "    for i, row in enumerate(csvreader):\n",
        "        if(i==0): continue\n",
        "        reviewText.append(row[2])\n",
        "        overall.append(int(row[3]))\n",
        "        productId.append(row[4])\n",
        "\n",
        "dataset = generate_tokenized_dataset(reviewText, overall, productId)\n",
        "np.save(\"/content/drive/MyDrive/CS470_team_2in1/dataset\"+\"/\"+\"rationale_preprocessed.npy\", dataset)"
      ],
      "metadata": {
        "id": "8KZ2TvJWuLeo"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}