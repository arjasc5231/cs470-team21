{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/arjasc5231/cs470-team21/blob/main/summarize_relevance_1129.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MEfVKG7jX9EV",
        "outputId": "d26faa6d-9945-414d-caa2-df6c668e52aa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# drive mount. colab에 내 구글 드라이브 연결\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### 아마존 데이터셋 분석을 위해 추가한 라이브러리\n",
        "import json\n",
        "import pickle\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "M5eaQ_pUYF95"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def combine_subtokens(tokens, relevance_score):\n",
        "    '''\n",
        "    combine subtokens to one word\n",
        "    '''\n",
        "\n",
        "    token = ['[CLS]', '[SEP]', '[MASK]', '[UNK]', '[PAD]']\n",
        "\n",
        "    for i in range(len(tokens)):\n",
        "        if tokens[i] in token: continue\n",
        "        \n",
        "        num = 1\n",
        "        word = tokens[i]\n",
        "        score = relevance_score[i]\n",
        "        while(tokens[i+1][0] == '#'):\n",
        "            word += tokens[i+1][2:]\n",
        "            score += relevance_score[i+1]\n",
        "            num += 1\n",
        "            if tokens[i][0] != '#':\n",
        "                tokens[i] = \"##\" + tokens[i]\n",
        "            i += 1\n",
        "        \n",
        "        tokens.append(word)\n",
        "        # average the relevance scores of subtokens\n",
        "        relevance_score.append(score/num)\n",
        "\n",
        "    # delete subtokens\n",
        "    word, score = [], []\n",
        "    for i in range(len(tokens)):\n",
        "        if tokens[i][0] != '#':\n",
        "            word.append(tokens[i])\n",
        "            score.append(relevance_score[i])\n",
        "    \n",
        "    return word, np.array(score)\n"
      ],
      "metadata": {
        "id": "Ll7GpoNHYJsb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from operator import pos\n",
        "from tqdm import tqdm\n",
        "def sum_relevance_score(records, occurence_rate=1, rating_weight=1, near=1, rationale = 0.99):\n",
        "    '''\n",
        "    combine the relevance scores of each words with some weights\n",
        "\n",
        "    Parameters\n",
        "    -------------------------------------------\n",
        "    records: numpy\n",
        "        [tokens, rating, product, output(0,1), neg_expl, pos_expl, \n",
        "         true_class(0,1), pred_class(0,1)]\n",
        "    \n",
        "    occurence_rate: float\n",
        "        threshold for rate of occurence / total records\n",
        "\n",
        "    rating_weight: float\n",
        "        weight for rating \n",
        "\n",
        "    near: \n",
        "\n",
        "\n",
        "    rationale: \n",
        "\n",
        "    Returns\n",
        "    -------------------------------------------\n",
        "    score_map: dictionary {'word': [positive_score, negative_score]}\n",
        "        a total score for each words\n",
        "\n",
        "    '''\n",
        "    pos_score_map, neg_score_map = {}, {}\n",
        "    pos_occur_map, neg_occur_map = {}, {}\n",
        "    pos_word, neg_word = {}, {}\n",
        "\n",
        "    tokens = ['[CLS]', '[SEP]', '[MASK]', '[UNK]', '[PAD]']\n",
        "\n",
        "    pos_num, neg_num = 0,0\n",
        "\n",
        "    for record in tqdm(records):\n",
        "\n",
        "        rate = (record[1] - 3) * rating_weight\n",
        "        if rate >= 0:   \n",
        "            pos_num += 1\n",
        "            score_map = pos_score_map\n",
        "            occur_map = pos_occur_map\n",
        "            highest_map = pos_word\n",
        "        else:           \n",
        "            neg_num += 1 \n",
        "            score_map = neg_score_map\n",
        "            occur_map = neg_occur_map\n",
        "            highest_map = neg_word\n",
        "\n",
        "        if record[0][0] != tokens[0] or record[0][-1] != tokens[1]: continue\n",
        "\n",
        "        word, negative_score = combine_subtokens(list(record[0]), list(record[4]))\n",
        "        word, positive_score = combine_subtokens(list(record[0]), list(record[5]))\n",
        "        \n",
        "        # normalize to range 0~1\n",
        "        positive_score = (positive_score - positive_score.min()) / (positive_score.max() - positive_score.min())\n",
        "        negative_score = (negative_score - negative_score.min()) / (negative_score.max() - negative_score.min())\n",
        "\n",
        "        assert len(word) == len(positive_score) == len(negative_score)\n",
        "\n",
        "        # relevance score을 주위 단어들로 분배\n",
        "        cache, score = {}, {}\n",
        "        for i in range(len(word)):\n",
        "            \n",
        "            if word[i] in tokens: continue\n",
        "\n",
        "            if word[i] not in occur_map:\n",
        "                occur_map[word[i]] = 0\n",
        "                score_map[word[i]] = 0\n",
        "\n",
        "            if word[i] not in cache:\n",
        "                occur_map[word[i]] += 1\n",
        "                cache[word[i]] = 0\n",
        "                score[word[i]] = 0\n",
        "            \n",
        "            cache[word[i]] += 1\n",
        "\n",
        "            # add the word with highest relevance score\n",
        "            if rate >= 0: \n",
        "                if i == np.argmax(positive_score):\n",
        "                    if word[i] not in highest_map: \n",
        "                        highest_map[word[i]] = 0\n",
        "                    highest_map[word[i]] += 1\n",
        "            else: \n",
        "                if i == np.argmax(negative_score):\n",
        "                    if word[i] not in highest_map: \n",
        "                        highest_map[word[i]] = 0\n",
        "                    highest_map[word[i]] += 1\n",
        "\n",
        "            if near >= 0:\n",
        "                min_index = max(0, i-near)\n",
        "                max_index = min(len(word), i+near)\n",
        "                for j in range(min_index, max_index):\n",
        "\n",
        "                    if i == j: continue\n",
        "\n",
        "                    # # pass built-in token\n",
        "                    if word[j] in tokens: continue\n",
        "\n",
        "                    if word[j] not in occur_map:\n",
        "                        occur_map[word[j]] = 0\n",
        "                        score_map[word[j]] = 0\n",
        "                        score[word[j]] = 0\n",
        "\n",
        "                    # # add score with weights\n",
        "                    # for p in range(len(scores)): \n",
        "                    #     score_map[word[j]][p] += rate * scores[p][i] / (max_index - min_index)\n",
        "\n",
        "                    # # positive review: add only positive score\n",
        "                    # # negative review: add only negative score\n",
        "                    # if rate >= 0: \n",
        "                    #     score_map[word[j]][0] += rate * scores[0][i] / (max_index - min_index)\n",
        "                    # else: \n",
        "                    #     score_map[word[j]][1] += rate * scores[1][i] / (max_index - min_index)\n",
        "\n",
        "                    if rate >= 0: \n",
        "                        score_map[word[j]] += rate * positive_score[i] / (max_index - min_index)\n",
        "                        score_map[word[j]] -= rate * negative_score[i] / (max_index - min_index)\n",
        "                    else: \n",
        "                        score_map[word[j]] += rate * negative_score[i] / (max_index - min_index)\n",
        "                        score_map[word[j]] -= rate * positive_score[i] / (max_index - min_index)\n",
        "\n",
        "                    continue\n",
        "\n",
        "\n",
        "            else:                     \n",
        "                if rate >= 0: \n",
        "                    score[word[i]] += rate * positive_score[i]\n",
        "                    # score[word[i]] -= rate * negative_score[i]\n",
        "                else: \n",
        "                    score[word[i]] += rate * negative_score[i]\n",
        "                    # score[word[i]] -= rate * positive_score[i]\n",
        "        for tok, num in cache.items():\n",
        "            score_map[tok] += score[tok] / num\n",
        "\n",
        "\n",
        "    print(f\"[sum_relevance_score] positive reviews: {pos_num}, negative reviews: {neg_num}\")\n",
        "    # delete words with less occurence\n",
        "    for word, occurence in pos_occur_map.items():\n",
        "        if occurence < pos_num * occurence_rate:\n",
        "            del pos_score_map[word]\n",
        "            continue\n",
        "\n",
        "        pos_score_map[word] /= occurence\n",
        "\n",
        "    for word, occurence in neg_occur_map.items():\n",
        "        if occurence < neg_num * occurence_rate:\n",
        "            del neg_score_map[word]\n",
        "            continue\n",
        "\n",
        "        neg_score_map[word] /= occurence\n",
        "\n",
        "    # print(f\"[sum_relevance_score] occurence - '[': {neg_occur_map[']']}, ']': {neg_occur_map['[']}\")\n",
        "\n",
        "    return pos_score_map, neg_score_map, pos_occur_map, neg_occur_map, pos_word, neg_word"
      ],
      "metadata": {
        "id": "uy6he1vEYL4m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "book_records_binary = np.load('/content/drive/MyDrive/CS470_team_2in1/colab/explanation/balanced_only1000model/amazon_book_expl-transformer_attribution-balanced_regression_only5000.npy', allow_pickle=True)"
      ],
      "metadata": {
        "id": "tbyLGtdIYQD_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pos_map, neg_map, pos_occur_map, neg_occur_map, pos_word, neg_word = sum_relevance_score(book_records_binary, occurence_rate = 0.01, rating_weight=1, near=-1)\n",
        "np.save(f\"/content/drive/MyDrive/CS470_team_2in1/colab/summarize/record_all\", sorted(pos_map.items(), key = lambda item: item[1], reverse=True)[:20])\n",
        "\n",
        "print(\"\\n\")\n",
        "print(sorted(pos_map.items(), key = lambda item: item[1], reverse=True)[:20])\n",
        "print(sorted(neg_map.items(), key = lambda item: item[1], reverse=True)[:20])\n",
        "print(\"\\n\")\n",
        "print(sorted(pos_occur_map.items(), key = lambda item: item[1], reverse=True)[:50])\n",
        "print(sorted(neg_occur_map.items(), key = lambda item: item[1], reverse=True)[:50])\n",
        "print(\"\\n\")\n",
        "print(sorted(pos_word.items(), key = lambda item: item[1], reverse=True)[:50])\n",
        "print(sorted(neg_word.items(), key = lambda item: item[1], reverse=True)[:50])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X-BbYM42OEd0",
        "outputId": "19b9fe98-4abe-461b-fd28-1b9a6b3269b3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5000/5000 [00:14<00:00, 352.66it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[sum_relevance_score] positive reviews: 4398, negative reviews: 602\n",
            "\n",
            "\n",
            "[('appalachian', 1.4964379239567358), ('bill', 0.9154671192555716), ('island', 0.7592160584541027), ('australian', 0.7546424165673082), ('local', 0.7391679271369712), ('ms', 0.7348293989712344), ('southern', 0.7330291703108752), ('rollins', 0.7197133592259561), ('americans', 0.7042976901605044), ('continent', 0.7006952267642037), ('country', 0.6914773223636017), ('!', 0.6885477217888775), ('this', 0.6749706002282332), ('trail', 0.6722960539221633), ('thank', 0.6603914788019147), ('south', 0.6603898556550765), ('sal', 0.6511935246525873), ('our', 0.6491117750878516), ('bones', 0.6459678574310012), ('sisters', 0.6435838545478827)]\n",
            "[('held', -0.047813241707357575), ('ago', -0.06049271221234306), ('eventually', -0.06359609385412059), ('fascinating', -0.06572562809976543), ('bring', -0.06804536871536565), ('able', -0.07149980057307953), ('fully', -0.07607188244945719), ('until', -0.0777788713637395), ('hoped', -0.07975481800445493), ('chance', -0.08105176357803706), ('haven', -0.08351452544574672), ('hoping', -0.08489190239636393), ('wonderful', -0.08510472475589262), ('soon', -0.09284388073062465), ('might', -0.09481205136487299), ('before', -0.09759653920871457), ('nice', -0.09820213020763129), ('amazing', -0.09868495231373257), ('grow', -0.09977580407096517), ('brought', -0.10066441755930172)]\n",
            "\n",
            "\n",
            "[('.', 4129), ('the', 3647), ('and', 3518), ('a', 3441), (',', 3241), ('to', 3210), ('of', 3189), ('this', 3142), ('i', 2991), ('book', 2903), ('is', 2811), ('it', 2769), (\"'\", 2753), ('in', 2707), ('that', 2324), ('for', 2124), ('read', 2097), ('s', 2068), ('with', 2034), ('but', 1878), ('was', 1817), ('as', 1804), ('on', 1770), ('-', 1742), ('not', 1626), ('you', 1576), ('are', 1533), ('have', 1530), ('his', 1511), ('one', 1503), ('about', 1433), ('an', 1407), ('be', 1405), ('t', 1399), ('he', 1366), ('from', 1300), ('all', 1297), ('who', 1288), ('at', 1271), ('so', 1223), ('by', 1184), ('\"', 1147), ('my', 1132), ('more', 1126), (')', 1118), ('has', 1096), ('(', 1095), ('!', 1093), ('or', 1080), ('what', 1077)]\n",
            "[('.', 597), ('the', 559), ('to', 520), ('and', 519), ('a', 511), (',', 511), ('of', 510), ('this', 497), ('i', 496), (\"'\", 482), ('book', 473), ('it', 467), ('in', 451), ('is', 434), ('that', 403), ('but', 357), ('s', 350), ('was', 348), ('for', 347), ('t', 346), ('not', 338), ('on', 318), ('-', 317), ('with', 311), ('be', 286), ('have', 281), ('as', 275), ('are', 267), ('\"', 262), ('read', 256), ('he', 253), ('you', 252), ('one', 249), ('at', 247), ('about', 245), ('just', 240), ('his', 240), ('so', 238), ('or', 228), ('if', 227), (')', 227), ('by', 224), ('(', 222), ('like', 219), ('an', 219), ('all', 213), ('would', 211), ('what', 209), ('there', 201), ('who', 191)]\n",
            "\n",
            "\n",
            "[('this', 445), ('book', 226), ('appalachian', 148), ('is', 139), ('!', 102), ('bill', 79), ('it', 68), ('i', 59), ('are', 36), ('bryson', 32), ('our', 31), ('country', 28), ('and', 28), ('trail', 28), ('these', 28), ('.', 28), ('us', 26), ('american', 24), ('my', 22), ('books', 22), ('read', 20), ('story', 20), ('bones', 20), ('we', 19), ('reader', 16), ('rollins', 16), ('has', 14), ('all', 13), ('island', 12), ('mr', 12), ('southern', 12), ('his', 12), ('cats', 12), ('panthers', 12), ('girls', 12), ('parrot', 12), ('series', 11), ('indian', 11), ('birdy', 11), ('cora', 11), ('peanuts', 11), ('sal', 10), ('woods', 10), (',', 10), ('australia', 10), ('stories', 10), ('sisters', 10), ('moore', 10), ('gibson', 10), ('have', 9)]\n",
            "[('book', 46), ('?', 41), ('!', 22), ('.', 22), ('\"', 16), ('not', 15), ('and', 13), ('just', 11), ('author', 9), (';', 9), ('story', 8), ('is', 8), (',', 7), ('boring', 7), ('piece', 7), ('novel', 6), ('blood', 5), ('crap', 5), ('bryson', 5), ('garbage', 4), ('american', 4), ('for', 4), ('hate', 4), ('humor', 4), ('like', 4), ('awful', 4), ('phrase', 3), ('this', 3), ('about', 3), ('bloody', 3), ('terrible', 3), ('horrible', 3), ('america', 3), ('dialogue', 3), ('useless', 3), ('a', 3), ('plot', 3), ('trash', 2), ('didn', 2), ('lie', 2), ('goldberg', 2), ('typing', 2), ('or', 2), ('junk', 2), ('vile', 2), ('writing', 2), ('brutal', 2), ('sheer', 2), ('totally', 2), ('gross', 2)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "product = {}\n",
        "for record in book_records_binary:\n",
        "    if record[2] not in product:\n",
        "        product[record[2]] = 0\n",
        "    product[record[2]] += 1\n",
        "print(len(product))\n",
        "print(product)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6S8HSZHTsUUi",
        "outputId": "ddea8dc2-2ff3-4c6f-df56-4e1485c1252f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "96\n",
            "{'0060755334': 256, '0060751967': 100, '0060740221': 867, '0060761288': 116, '0060760060': 1, '0060758716': 7, '006073731X': 26, '0060747293': 12, '006076032X': 1, '0060763426': 42, '0060763760': 9, '0060758317': 2, '0060763868': 2, '0060756632': 13, '0060762489': 1, '0060753854': 5, '0060759836': 296, '0060745053': 67, '0060755504': 12, '0060761334': 48, '0060748125': 88, '0060760958': 64, '0060763515': 13, '0060763620': 63, '0060760885': 223, '0060760257': 48, '0060757612': 29, '0060761857': 46, '0060763116': 24, '006076287X': 43, '0060765410': 12, '0060760281': 10, '006076208X': 34, '0060762519': 1, '0060745258': 7, '0060755342': 15, '0060754001': 10, '0060758465': 1, '0060761881': 2, '0060760664': 9, '0060753455': 1, '0060761784': 27, '0060751002': 25, '0060739428': 113, '0060763876': 452, '0060765127': 7, '0060759682': 7, '0060765712': 116, '0060765402': 50, '0060759704': 5, '0060756047': 6, '0060765364': 37, '0060765704': 6, '0060764961': 11, '0060763272': 47, '0060755229': 24, '0060754338': 6, '0060756705': 9, '0060751525': 2, '0060758759': 31, '0060762365': 5, '0060763442': 30, '0060766913': 9, '0060761512': 2, '0060747811': 31, '0060766212': 72, '0060763469': 6, '0060763957': 175, '0060756675': 131, '0060753994': 3, '0060766328': 30, '0060773758': 258, '0060772611': 7, '0060763450': 141, '0060761539': 7, '0060762276': 19, '0060760176': 49, '0060766158': 4, '0060766603': 25, '0060762055': 93, '0060763671': 13, '0060739959': 7, '0060772913': 83, '0060773170': 64, '006075656X': 42, '0060775599': 8, '0060758929': 7, '0060775734': 11, '0060740957': 24, '0060773162': 6, '0060748206': 6, '0060774703': 8, '0060765380': 2, '0060773367': 8, '0060766867': 2, '0060764694': 5}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "product_name = '0002051850'\n",
        "record_0001384198 = []\n",
        "for record in book_records_binary:\n",
        "    if record[2] == product_name:\n",
        "        record_0001384198.append(record)\n",
        "\n",
        "pos_map, neg_map, pos_occur_map, neg_occur_map, pos_word, neg_word = sum_relevance_score(record_0001384198, occurence_rate = 0.01, rating_weight=1, near=-1)\n",
        "np.save(f\"/content/drive/MyDrive/CS470_team_2in1/colab/summarize/record_{product_name}\", sorted(pos_map.items(), key = lambda item: item[1], reverse=True)[:20])\n",
        "\n",
        "print(\"\\n\")\n",
        "print(sorted(pos_map.items(), key = lambda item: item[1], reverse=True)[:20])\n",
        "print(sorted(neg_map.items(), key = lambda item: item[1], reverse=True)[:20])\n",
        "print(\"\\n\")\n",
        "print(sorted(pos_occur_map.items(), key = lambda item: item[1], reverse=True)[:20])\n",
        "print(sorted(neg_occur_map.items(), key = lambda item: item[1], reverse=True)[:20])\n",
        "print(\"\\n\")\n",
        "print(sorted(pos_word.items(), key = lambda item: item[1], reverse=True)[:20])\n",
        "print(sorted(neg_word.items(), key = lambda item: item[1], reverse=True)[:20])     "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rrOPe6q6s5U-",
        "outputId": "f1013207-20b7-4130-8fe8-ed33df3069d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "0it [00:00, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[sum_relevance_score] positive reviews: 0, negative reviews: 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "[]\n",
            "[]\n",
            "\n",
            "\n",
            "[]\n",
            "[]\n",
            "\n",
            "\n",
            "[]\n",
            "[]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "book_records_regression = np.load('/content/drive/MyDrive/CS470_team_2in1/colab/explanation/amazon_book_expl-transformer_attribution-regression_only1000.npy', allow_pickle=True)"
      ],
      "metadata": {
        "id": "jWZBZEO5f28q",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "2671be4f-6083-4074-8738-48f3b20d0276"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-781df838059f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbook_records_regression\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/MyDrive/CS470_team_2in1/colab/explanation/amazon_book_expl-transformer_attribution-regression_only1000.npy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_pickle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding)\u001b[0m\n\u001b[1;32m    415\u001b[0m             \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    416\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 417\u001b[0;31m             \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstack\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menter_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos_fspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    418\u001b[0m             \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/CS470_team_2in1/colab/explanation/amazon_book_expl-transformer_attribution-regression_only1000.npy'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pos_map, neg_map, pos_occur_map, neg_occur_map, pos_word, neg_word = sum_relevance_score(book_records_regression, occurence_rate = 0.01, rating_weight=1, near=3)\n",
        "\n",
        "print(\"\\n\")\n",
        "print(sorted(pos_map.items(), key = lambda item: item[1], reverse=True)[:20])\n",
        "print(sorted(neg_map.items(), key = lambda item: item[1], reverse=True)[:20])\n",
        "print(\"\\n\")\n",
        "print(sorted(pos_occur_map.items(), key = lambda item: item[1], reverse=True)[:20])\n",
        "print(sorted(neg_occur_map.items(), key = lambda item: item[1], reverse=True)[:20])\n",
        "print(\"\\n\")\n",
        "print(sorted(pos_word.items(), key = lambda item: item[1], reverse=True)[:20])\n",
        "print(sorted(neg_word.items(), key = lambda item: item[1], reverse=True)[:20])"
      ],
      "metadata": {
        "id": "I9pqO1Zkuo8C"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}