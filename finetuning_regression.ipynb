{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_ySktVCHFDLy"
      },
      "outputs": [],
      "source": [
        "# drive mount. colab에 내 구글 드라이브 연결\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "!pip install transformers &> /dev/null\n",
        "!pip install captum &> /dev/null"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9lpKlirkFOiS"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import TensorDataset, random_split\n",
        "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "from transformers import BertTokenizer\n",
        "from transformers import AutoTokenizer  # bert 모델에 따라 알맞은 tokenizer를 자동으로 로드\n",
        "from transformers import BertForSequenceClassification\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "\n",
        "from captum.attr import visualization # XAI관련 라이브러리의 시각화 함수\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "import os\n",
        "import json\n",
        "import pickle\n",
        "import numpy as np\n",
        "import random\n",
        "from collections import OrderedDict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ESXImwVKeEFe"
      },
      "outputs": [],
      "source": [
        "# GPU 찾기. 없으면 CPU로 동작\n",
        "if torch.cuda.is_available():  \n",
        "    device = torch.device(\"cuda\")\n",
        "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
        "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")\n",
        "\n",
        "# check torch is available\n",
        "print(torch.__version__)\n",
        "print(torch.tensor([1.0, 2.0]).cuda())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 부정적인 단어를 학습하려면 negative_regression=True\n",
        "def development(model_saving_name, negative_regression=False):\n",
        "  \n",
        "\n",
        "  ######## generate dataloader ########\n",
        "  def edit_data(path, random=False):\n",
        "    input_ids, attention_masks, ratings, product = np.load(path, allow_pickle=True)\n",
        "    if negative_regression:\n",
        "      ratings -= 6\n",
        "      ratings *= -1\n",
        "    ratings = np.expand_dims(ratings, axis=-1)\n",
        "\n",
        "    data_num = len(input_ids)\n",
        "    batch_size = 8\n",
        "\n",
        "    # Convert to tensors and make dataset\n",
        "    input_ids = torch.cat(input_ids.tolist(), dim=0)\n",
        "    attention_masks = torch.cat(attention_masks.tolist(), dim=0)\n",
        "    labels = torch.tensor(ratings.tolist(), dtype=torch.float32)\n",
        "    dataset = TensorDataset(input_ids, attention_masks, labels)\n",
        "    if random: dataloader = DataLoader(dataset, sampler = RandomSampler(dataset), batch_size = batch_size)\n",
        "    else: dataloader = DataLoader(dataset, sampler = SequentialSampler(dataset), batch_size = batch_size)\n",
        "\n",
        "    return dataloader,data_num\n",
        "  \n",
        "  train_dataloader,train_size = edit_data(\"/content/drive/MyDrive/CS470_team_2in1/dataset/amazon_book_train_rev.npy\", random=True)\n",
        "  val_dataloader,val_size = edit_data(\"/content/drive/MyDrive/CS470_team_2in1/dataset/amazon_book_val_rev.npy\", random=False)\n",
        "  test_dataloader,test_size = edit_data(\"/content/drive/MyDrive/CS470_team_2in1/dataset/amazon_book_test_rev.npy\", random=False)\n",
        "  ##################################\n",
        "\n",
        "\n",
        "\n",
        "  ######## prepare training ########\n",
        "  model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=1)\n",
        "  model.cuda()\n",
        "\n",
        "  optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, eps=1e-8)\n",
        "\n",
        "  epochs = 4 # The BERT authors recommend between 2 and 4\n",
        "  total_steps = len(train_dataloader) * epochs\n",
        "\n",
        "  scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps = 0, num_training_steps = total_steps)\n",
        "\n",
        "  criterion = torch.nn.MSELoss(reduction='none')\n",
        "  ##################################\n",
        "\n",
        "\n",
        "\n",
        "  ######## #train/val/test #########\n",
        "  results = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': []}\n",
        "\n",
        "  best_val_acc = 0\n",
        "  best_val_loss = float('inf')\n",
        "  best_model_state_dict = None\n",
        "\n",
        "  for epoch in range(0, epochs):\n",
        "\n",
        "      print('\\n======== Epoch {:} / {:} ========\\n'.format(epoch + 1, epochs))\n",
        "\n",
        "      # train\n",
        "      e_train_loss = 0\n",
        "      e_train_acc = 0\n",
        "      model.train()\n",
        "      for batch in train_dataloader:\n",
        "          b_input_ids, b_input_mask, b_labels = batch\n",
        "          b_input_ids, b_input_mask, b_labels = b_input_ids.to(device), b_input_mask.to(device), b_labels.to(device)\n",
        "\n",
        "          preds = model(input_ids=b_input_ids, attention_mask=b_input_mask)[0]\n",
        "          #e_train_acc += accuracy_score(preds.argmax(dim=1).cpu(), b_labels.cpu(), normalize=False)\n",
        "\n",
        "          loss = criterion(preds, b_labels).sum()\n",
        "          e_train_loss += loss.item()\n",
        "\n",
        "          torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0) # Clip the norm of the gradients to 1.0. This is to help prevent the \"exploding gradients\" problem.\n",
        "\n",
        "          optimizer.zero_grad()\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "\n",
        "          scheduler.step()\n",
        "      e_train_loss /= train_size\n",
        "      #e_train_acc /= train_size\n",
        "      results['train_loss'].append(e_train_loss)\n",
        "      #results['train_acc'].append(e_train_acc)\n",
        "\n",
        "\n",
        "      # validation\n",
        "      model.eval()\n",
        "      with torch.no_grad():\n",
        "        e_val_loss = 0\n",
        "        e_val_acc = 0\n",
        "        for batch in val_dataloader:\n",
        "          b_input_ids, b_input_mask, b_labels = batch\n",
        "          b_input_ids, b_input_mask, b_labels = b_input_ids.to(device), b_input_mask.to(device), b_labels.to(device)\n",
        "\n",
        "          preds = model(input_ids=b_input_ids, attention_mask=b_input_mask)[0]\n",
        "          #e_val_acc += accuracy_score(preds.argmax(dim=1).cpu(), b_labels.cpu(), normalize=False)\n",
        "\n",
        "          loss = criterion(preds, b_labels).sum()\n",
        "          e_val_loss += loss.item()\n",
        "        e_val_loss /= val_size\n",
        "        #e_val_acc /= val_size\n",
        "        results[\"val_acc\"].append(e_val_acc)\n",
        "        #results[\"val_loss\"] = e_val_loss\n",
        "\n",
        "\n",
        "      # save best model weights\n",
        "      if best_val_loss > e_val_loss:\n",
        "        best_val_loss = e_val_loss\n",
        "        best_model_state_dict = OrderedDict({k: v.cpu() for k, v in model.state_dict().items()})\n",
        "      \n",
        "      \n",
        "      print('train loss:',e_train_loss)\n",
        "      #print('train acc:',e_train_acc)\n",
        "      print('val loss:',e_val_loss)\n",
        "      #print('val acc:',e_val_acc)\n",
        "\n",
        "  #load best validation loss model and save\n",
        "  model.load_state_dict(best_model_state_dict)\n",
        "  torch.save(best_model_state_dict, \"/content/drive/MyDrive/CS470_team_2in1/colab/model/\"+model_saving_name)\n",
        "\n",
        "  # test\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "    e_test_loss = 0\n",
        "    e_test_acc = 0\n",
        "    for batch in test_dataloader:\n",
        "      b_input_ids, b_input_mask, b_labels = batch\n",
        "      b_input_ids, b_input_mask, b_labels = b_input_ids.to(device), b_input_mask.to(device), b_labels.to(device)\n",
        "\n",
        "      preds = model(input_ids=b_input_ids, attention_mask=b_input_mask)[0]\n",
        "      #e_test_acc += accuracy_score(preds.argmax(dim=1).cpu(), b_labels.cpu(), normalize=False)\n",
        "\n",
        "      loss = criterion(preds, b_labels).sum()\n",
        "      e_test_loss += loss.item()\n",
        "    e_test_loss /= test_size\n",
        "    #e_test_acc /= test_size\n",
        "    print('test loss:',e_test_loss)\n",
        "    # print(e_test_acc)"
      ],
      "metadata": {
        "id": "Ulj7l5y6LRhU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "development(model_saving_name=\"positive_regression.pt\", negative_regression=False)\n",
        "development(model_saving_name=\"negative_regression.pt\", negative_regression=True)"
      ],
      "metadata": {
        "id": "zKzgLXyPNPNv"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}