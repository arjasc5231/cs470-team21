{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "YCdGaMuy56TA",
        "outputId": "540081a0-7cf8-4766-b2c1-98237ba71c42"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "Cloning into 'Transformer-Explainability'...\n",
            "remote: Enumerating objects: 377, done.\u001b[K\n",
            "remote: Counting objects: 100% (152/152), done.\u001b[K\n",
            "remote: Compressing objects: 100% (78/78), done.\u001b[K\n",
            "remote: Total 377 (delta 127), reused 74 (delta 74), pack-reused 225\u001b[K\n",
            "Receiving objects: 100% (377/377), 3.83 MiB | 37.39 MiB/s, done.\n",
            "Resolving deltas: 100% (190/190), done.\n"
          ]
        }
      ],
      "source": [
        "# drive mount. colab에 내 구글 드라이브 연결\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# clone git repo\n",
        "!git clone https://github.com/hila-chefer/Transformer-Explainability.git\n",
        "\n",
        "# change directory\n",
        "import os\n",
        "os.chdir(f'./Transformer-Explainability')\n",
        "\n",
        "# install libraries\n",
        "!pip install -r requirements.txt &> /dev/null\n",
        "!pip install captum &> /dev/null\n",
        "!pip install matplotlib==3.2.2 &> /dev/null"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HEcRNC_VXjsQ"
      },
      "source": [
        "# 새 섹션"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "4-XGl_Zw6Aht"
      },
      "outputs": [],
      "source": [
        "### transformer 및 설명 생성을 위한 라이브러리\n",
        "import torch\n",
        "\n",
        "from transformers import BertTokenizer\n",
        "from transformers import AutoTokenizer  # bert 모델에 따라 알맞는 tokenizer를 자동으로 로드\n",
        "\n",
        "from BERT_explainability.modules.BERT.ExplanationGenerator import Generator\n",
        "from BERT_explainability.modules.BERT.BertForSequenceClassification import BertForSequenceClassification\n",
        "\n",
        "from captum.attr import visualization # XAI관련 라이브러리의 시각화 함수\n",
        "\n",
        "\n",
        "### 아마존 데이터셋 분석을 위해 추가한 라이브러리\n",
        "import json\n",
        "import pickle\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "VakYjrkC6C3S",
        "outputId": "e860824a-a59a-4739-b2a4-376a1914fc38"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nmodel = BertForSequenceClassification.from_pretrained(\"textattack/bert-base-uncased-SST-2\").to(\"cuda\")\\ntokenizer = AutoTokenizer.from_pretrained(\"textattack/bert-base-uncased-SST-2\")\\n\\nmodel.eval()  # model을 evaluation mode로 전환\\nexplanations = Generator(model)  # 설명 생성 객체 초기화\\n'"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\"\"\"\n",
        "model = BertForSequenceClassification.from_pretrained(\"textattack/bert-base-uncased-SST-2\").to(\"cuda\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"textattack/bert-base-uncased-SST-2\")\n",
        "\n",
        "model.eval()  # model을 evaluation mode로 전환\n",
        "explanations = Generator(model)  # 설명 생성 객체 초기화\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "eJg3oRhkLPUc"
      },
      "outputs": [],
      "source": [
        "def preprocess_amazon(json_filename = 'AMAZON_FASHION_5.json'):\n",
        "  review_texts = []\n",
        "  ratings = []\n",
        "  products = []\n",
        "\n",
        "  root = '/content/drive/MyDrive/CS470_team_2in1'\n",
        "  with open(root+'/'+'dataset'+'/'+json_filename, \"r\") as json_file:\n",
        "    for line in json_file:\n",
        "      info = json.loads(line)\n",
        "\n",
        "      try:\n",
        "        review_text = info[\"reviewText\"]\n",
        "        rating = int(info[\"overall\"])\n",
        "        product = info[\"asin\"]\n",
        "      except:\n",
        "        continue\n",
        "      \n",
        "      review_texts.append(review_text)\n",
        "      ratings.append(rating)\n",
        "      products.append(product)\n",
        "\n",
        "  print(f'collected {len(review_texts)} datas')\n",
        "  return review_texts, ratings, products"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "d8OtmhzkZgEs"
      },
      "outputs": [],
      "source": [
        "# 하나의 문장에 대해 토큰,예측,설명을 생성\n",
        "def interpret_sentence(model, expl_generator, tokenizer, sentence, target_class=None):\n",
        "  # tokenize 해서 token id와 attention mask를 얻기\n",
        "  encoding = tokenizer(sentence, return_tensors='pt')\n",
        "  # 만약 token 개수가 model input dim을 넘는다면 아래 코드 사용. max_length는 몇이어야 할지 모르겠다. 모델 학습때는 이걸 사용하려나\n",
        "  # encoding = tokenizer(sentence, max_length=100, padding=\"max_length\", truncation=True, return_tensors='pt')\n",
        "  input_ids = encoding['input_ids'].to(\"cuda\")\n",
        "  attention_mask = encoding['attention_mask'].to(\"cuda\")\n",
        "  tokens = tokenizer.convert_ids_to_tokens(input_ids.flatten())\n",
        "\n",
        "  # 모델 출력\n",
        "  output = torch.nn.functional.softmax(model(input_ids=input_ids, attention_mask=attention_mask)[0], dim=-1)\n",
        "  pred_class = output.argmax(dim=-1).item()\n",
        "  output = output.detach().cpu().numpy()\n",
        "  if target_class==None: target_class = pred_class  # 일단은 예측 라벨에 대해서 설명 생성\n",
        "\n",
        "  # 설명 생성\n",
        "  expl = expl_generator.generate_LRP(input_ids=input_ids, attention_mask=attention_mask, start_layer=11, index=target_class)[0]\n",
        "  expl = expl.detach().cpu().numpy()\n",
        "  expl = (expl - expl.min()) / (expl.max() - expl.min()) # normalize scores\n",
        "  # normalize 방식이 https://captum.ai/tutorials/IMDB_TorchText_Interpret 랑 다른데? 링크는 l2, 코드는 최댓값을 1로.\n",
        "\n",
        "  return tokens, output, expl, pred_class, target_class\n",
        "\n",
        "\n",
        "\n",
        "# 전체 데이터셋에 대해 설명 생성\n",
        "def interpret_all_sentences(model, expl_generator, tokenizer, sentences, ratings, products):\n",
        "  records = []\n",
        "  data_num = len(sentences)\n",
        "\n",
        "  for i in range(data_num):\n",
        "    sentence = sentences[i]\n",
        "    rating = ratings[i]\n",
        "    product = products[i]\n",
        "\n",
        "    # 문장에 대한 설명 생성\n",
        "    tokens, output, expl, pred_class, target_class = interpret_sentence(model, expl_generator, tokenizer, sentence)\n",
        "\n",
        "    # true label 판단\n",
        "    true_class = 1 if rating>=3.0 else 0\n",
        "\n",
        "    records.append([tokens, rating, product, output, expl, true_class, pred_class, target_class])\n",
        "\n",
        "  return np.array(records)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "3ck-bny5qtnk"
      },
      "outputs": [],
      "source": [
        "# interpret_all_sentences의 출력 또는 그 출력을 저장한 파일경로로부터 설명 생성\n",
        "def visualize_expl(records, visualization_num):\n",
        "  # record가 파일 경로일 경우 불러오기\n",
        "  if isinstance(records, str): records = np.load(records, allow_pickle=True)\n",
        "\n",
        "  vis_datas = []\n",
        "  for i in range(visualization_num):\n",
        "    tokens, rating, product, output, expl, true_class, pred_class, target_class = records[i]\n",
        "    if target_class==0: expl *= -1  # negative일 경우 빨간색으로 visualize하기 위해.\n",
        "\n",
        "    # visualization 객체 생성해서 추가\n",
        "    vis_datas.append(visualization.VisualizationDataRecord(\n",
        "                                  expl,\n",
        "                                  output[0][pred_class],\n",
        "                                  pred_class,\n",
        "                                  true_class,\n",
        "                                  target_class,\n",
        "                                  expl.sum(),       \n",
        "                                  tokens,\n",
        "                                  1))\n",
        "  \n",
        "  # visualize\n",
        "  visualization.visualize_text(vis_datas)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "yTLR2iHXy2Dl"
      },
      "outputs": [],
      "source": [
        "def generate_LRP_amazon(record_save_dir, json_filename):\n",
        "  # preprocess\n",
        "  review_texts, ratings, products = preprocess_amazon(json_filename)\n",
        "\n",
        "  # load model, tokenizer, explation_generator\n",
        "  model = BertForSequenceClassification.from_pretrained(\"textattack/bert-base-uncased-SST-2\").to(\"cuda\")\n",
        "  model.eval()  # model을 evaluation mode로 전환\n",
        "  tokenizer = AutoTokenizer.from_pretrained(\"textattack/bert-base-uncased-SST-2\")\n",
        "  epxl_generator = Generator(model)  # 설명 생성 객체\n",
        "\n",
        "  # 설명 생성 및 저장\n",
        "  records = interpret_all_sentences(model, epxl_generator, tokenizer, review_texts, ratings, products)\n",
        "  np.save(record_save_dir+\"/\"+\"LRP_amazon.npy\", records)\n",
        "  #with open(record_save_dir+\"/\"+\"LRP_amazon\",\"wb\") as f: pickle.dump(records, f)\n",
        "\n",
        "  # visualize\n",
        "  visualize_expl(records,10,1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "a4x3BSOI07Yd",
        "outputId": "f4ff8ee2-cd0b-45d5-9a85-cce4d986c8e2"
      },
      "outputs": [
        {
          "ename": "TypeError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-e518ea26ec1e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#generate_LRP_amazon('/content/drive/MyDrive/CS470_team_2in1/colab', 'AMAZON_FASHION_5.json')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mvisualize_expl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/MyDrive/CS470_team_2in1/colab/LRP_amazon.npy'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m: visualize_expl() takes 2 positional arguments but 3 were given"
          ]
        }
      ],
      "source": [
        "#generate_LRP_amazon('/content/drive/MyDrive/CS470_team_2in1/colab', 'AMAZON_FASHION_5.json')\n",
        "visualize_expl('/content/drive/MyDrive/CS470_team_2in1/colab/LRP_amazon.npy',100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "_AgEelxj5cY6"
      },
      "outputs": [],
      "source": [
        "records = np.load('/content/drive/MyDrive/CS470_team_2in1/colab/LRP_amazon.npy', allow_pickle=True)\n",
        "print(records[:10])"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}